{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Spectral Metric","text":"<p>This library provides an implementation of CSG, from CVPR 2019 paper: Spectral Metric for Dataset Complexity Assessment.</p> <p>By using <code>spectral-metric</code> you can learn more about your dataset without training a model.</p> <p>Spectral metric is domain agnostic and fast compared to previous state-of-the-art.<sup>1</sup></p> <p>Spectral metric in action:</p> <ol> <li>\ud83e\udd17 HuggingFace Space</li> <li>In-depth analysis of CLINC-150</li> </ol>"},{"location":"#support","title":"Support","text":"<p>For support, please submit an issue! This project is actively maintained.</p> <ol> <li> <p>Lorena et al. provide a good survey of the dataset complexity domain.\u00a0\u21a9</p> </li> </ol>"},{"location":"get-started/","title":"Get Started","text":""},{"location":"get-started/#installation","title":"Installation","text":"<p>Our package is available on Pypi:</p> <p><code>pip install spectral-metric</code></p>"},{"location":"get-started/#how-to-use","title":"How to use","text":"<p>This library works with two arrays, the features and the labels. The features are ideally normalized and have low-dimensionality. In the paper, we use t-SNE to reduce the dimensionality.</p> <pre><code>from spectral_metric.estimator import CumulativeGradientEstimator\nfrom spectral_metric.visualize import make_graph\n\nX, y = ...  # Your dataset with shape [N, ?], [N]\nestimator = CumulativeGradientEstimator(M_sample=250, k_nearest=5)\nestimator.fit(data=X, target=y)\ncsg = estimator.csg  # The actual complexity values.\nestimator.evals, estimator.evecs  # The eigenvalues and vectors.\n\n# You can plot the dataset with:\nmake_graph(estimator.difference, title=\"Your dataset\", classes=[\"A\", \"B\", \"C\"])\n</code></pre> <p>Below we can see the results on MNIST, CIFAR10 and MIO-TCD</p> <p></p>"},{"location":"api/estimator/","title":"API Reference for <code>CumulativeGradientEstimator</code>","text":"<p>             Bases: <code>object</code></p> Source code in <code>spectral_metric/estimator.py</code> <pre><code>class CumulativeGradientEstimator(object):\n    def __init__(self, M_sample=250, k_nearest=3, distance=\"euclidean\"):\n        \"\"\"\n        The Cumulative Gradient Estimator, estimates the complexity of a dataset.\n        Args:\n            M_sample (int): Number of sample per class to use\n            k_nearest (int): Number of neighbours to look to compute $P(C_c \\vert x)$.\n            distance: name of the distance to use.\n        \"\"\"\n        self.M_sample = M_sample\n        self.k_nearest = k_nearest\n        self.distance = distance\n\n    def fit(self, data, target):\n        \"\"\"\n        Estimate the CSG metric from the data\n        Args:\n            data: data samples, ndarray (n_samples, n_features)\n            target: target samples, ndarray (n_samples)\n        \"\"\"\n        np.random.seed(None)\n        data_x = data.copy()\n        self.n_class = np.max(target) - min(0, np.min(target)) + 1\n\n        # Do class sampling\n        class_samples, self.class_indices = find_samples(\n            data_x, target, self.n_class, M=self.M_sample\n        )\n\n        self.compute(data_x, target, class_samples)\n        return self\n\n    def compute(self, data, target, class_samples):\n        \"\"\"\n        Compute the difference matrix and the eigenvalues\n        Args:\n            data: data samples, ndarray (n_samples, n_features)\n            target: target samples, ndarray (n_samples)\n            class_samples : class samples, Dict[class_idx, Array[M, n_features]]\n        \"\"\"\n        # Compute E_{p(x\\mid C_i)} [p(x\\mid C_j)]\n        self.S, self.similarity_arrays = compute_expectation_with_monte_carlo(\n            data,\n            target,\n            class_samples,\n            class_indices=self.class_indices,\n            n_class=self.n_class,\n            k_nearest=self.k_nearest,\n            distance=self.distance,\n        )\n\n        # Compute the D matrix\n        self.W = np.eye(self.n_class)\n        for i, j in product(range(self.n_class), range(self.n_class)):\n            self.W[i, j] = 1 - scipy.spatial.distance.braycurtis(self.S[i], self.S[j])\n\n        self.difference = 1 - self.W\n\n        # Get the Laplacian and its eigen values\n        self.L_mat, dd = laplacian(self.W, False, True)\n        try:\n            self.evals, self.evecs = np.linalg.eigh(self.L_mat)\n            self.csg = self._csg_from_evals(self.evals)\n        except LinAlgError as e:\n            log.warning(f\"{str(e)}; assigning `evals,evecs,csg` to NaN\")\n            self.evals = np.ones([self.n_class]) * np.nan\n            self.evecs = np.ones([self.n_class, self.n_class]) * np.nan\n            self.csg = np.nan\n\n    def _csg_from_evals(self, evals: np.ndarray) -&gt; float:\n        # [n_class]\n        grads = evals[1:] - evals[:-1]\n        ratios = grads / (np.array([list(reversed(range(1, grads.shape[-1] + 1)))]) + 1)\n        csg: float = np.maximum.accumulate(ratios, -1).sum(1)\n        return csg\n</code></pre>"},{"location":"api/estimator/#spectral_metric.estimator.CumulativeGradientEstimator.__init__","title":"<code>__init__(M_sample=250, k_nearest=3, distance='euclidean')</code>","text":"<p>The Cumulative Gradient Estimator, estimates the complexity of a dataset. Args:     M_sample (int): Number of sample per class to use     k_nearest (int): Number of neighbours to look to compute \\(P(C_c \u000bert x)\\).     distance: name of the distance to use.</p> Source code in <code>spectral_metric/estimator.py</code> <pre><code>def __init__(self, M_sample=250, k_nearest=3, distance=\"euclidean\"):\n    \"\"\"\n    The Cumulative Gradient Estimator, estimates the complexity of a dataset.\n    Args:\n        M_sample (int): Number of sample per class to use\n        k_nearest (int): Number of neighbours to look to compute $P(C_c \\vert x)$.\n        distance: name of the distance to use.\n    \"\"\"\n    self.M_sample = M_sample\n    self.k_nearest = k_nearest\n    self.distance = distance\n</code></pre>"},{"location":"api/estimator/#spectral_metric.estimator.CumulativeGradientEstimator.compute","title":"<code>compute(data, target, class_samples)</code>","text":"<p>Compute the difference matrix and the eigenvalues Args:     data: data samples, ndarray (n_samples, n_features)     target: target samples, ndarray (n_samples)     class_samples : class samples, Dict[class_idx, Array[M, n_features]]</p> Source code in <code>spectral_metric/estimator.py</code> <pre><code>def compute(self, data, target, class_samples):\n    \"\"\"\n    Compute the difference matrix and the eigenvalues\n    Args:\n        data: data samples, ndarray (n_samples, n_features)\n        target: target samples, ndarray (n_samples)\n        class_samples : class samples, Dict[class_idx, Array[M, n_features]]\n    \"\"\"\n    # Compute E_{p(x\\mid C_i)} [p(x\\mid C_j)]\n    self.S, self.similarity_arrays = compute_expectation_with_monte_carlo(\n        data,\n        target,\n        class_samples,\n        class_indices=self.class_indices,\n        n_class=self.n_class,\n        k_nearest=self.k_nearest,\n        distance=self.distance,\n    )\n\n    # Compute the D matrix\n    self.W = np.eye(self.n_class)\n    for i, j in product(range(self.n_class), range(self.n_class)):\n        self.W[i, j] = 1 - scipy.spatial.distance.braycurtis(self.S[i], self.S[j])\n\n    self.difference = 1 - self.W\n\n    # Get the Laplacian and its eigen values\n    self.L_mat, dd = laplacian(self.W, False, True)\n    try:\n        self.evals, self.evecs = np.linalg.eigh(self.L_mat)\n        self.csg = self._csg_from_evals(self.evals)\n    except LinAlgError as e:\n        log.warning(f\"{str(e)}; assigning `evals,evecs,csg` to NaN\")\n        self.evals = np.ones([self.n_class]) * np.nan\n        self.evecs = np.ones([self.n_class, self.n_class]) * np.nan\n        self.csg = np.nan\n</code></pre>"},{"location":"api/estimator/#spectral_metric.estimator.CumulativeGradientEstimator.fit","title":"<code>fit(data, target)</code>","text":"<p>Estimate the CSG metric from the data Args:     data: data samples, ndarray (n_samples, n_features)     target: target samples, ndarray (n_samples)</p> Source code in <code>spectral_metric/estimator.py</code> <pre><code>def fit(self, data, target):\n    \"\"\"\n    Estimate the CSG metric from the data\n    Args:\n        data: data samples, ndarray (n_samples, n_features)\n        target: target samples, ndarray (n_samples)\n    \"\"\"\n    np.random.seed(None)\n    data_x = data.copy()\n    self.n_class = np.max(target) - min(0, np.min(target)) + 1\n\n    # Do class sampling\n    class_samples, self.class_indices = find_samples(\n        data_x, target, self.n_class, M=self.M_sample\n    )\n\n    self.compute(data_x, target, class_samples)\n    return self\n</code></pre>"},{"location":"api/lib/","title":"Core functions","text":"<p>Reference for <code>compute_expectation_with_monte_carlo</code></p> <p>Compute \\(E_{p(x | C_i)} [p(x | C_j)]\\) for all classes from samples with a monte carlo estimator. Args:     data: [num_samples, n_features], the inputs     target: [num_samples], the classes     class_samples: [n_class, M, n_features], the M samples per class     class_indices: [n_class, indices], the indices of samples per class     n_class: The number of classes     k_nearest: The number of neighbors for k-NN     distance: Which distance metric to use</p> <p>Returns:</p> Name Type Description <code>expectation</code> <code>Array</code> <p>[n_class, n_class], matrix with probabilities</p> <code>similarity_arrays</code> <code>Dict[int, Dict[int, SimilarityArrays]]</code> <p>[n_class, M, SimilarityArrays], dict of arrays with kNN class                 proportions, raw and normalized by the Parzen-window,                 accessed via class and sample indices</p> Source code in <code>spectral_metric/lib.py</code> <pre><code>def compute_expectation_with_monte_carlo(\n    data: Array,\n    target: Array,\n    class_samples: Dict[int, Array],\n    class_indices: Dict[int, Array],\n    n_class: int,\n    k_nearest=5,\n    distance: str = \"euclidean\",\n) -&gt; Tuple[Array, Dict[int, Dict[int, SimilarityArrays]]]:\n    \"\"\"\n    Compute $E_{p(x | C_i)} [p(x | C_j)]$ for all classes from samples\n    with a monte carlo estimator.\n    Args:\n        data: [num_samples, n_features], the inputs\n        target: [num_samples], the classes\n        class_samples: [n_class, M, n_features], the M samples per class\n        class_indices: [n_class, indices], the indices of samples per class\n        n_class: The number of classes\n        k_nearest: The number of neighbors for k-NN\n        distance: Which distance metric to use\n\n    Returns:\n        expectation: [n_class, n_class], matrix with probabilities\n        similarity_arrays: [n_class, M, SimilarityArrays], dict of arrays with kNN class\n                            proportions, raw and normalized by the Parzen-window,\n                            accessed via class and sample indices\n\n    \"\"\"\n\n    def get_volume(dt):\n        # [k_nearest, ?]\n        # Compute the volume of the parzen-window\n        dst = (np.abs(dt[1:] - dt[0]).max(0) * 2).prod()\n        # Minimum is 1e-4 to avoid division per 0\n        res = max(1e-4, dst)\n        return res\n\n    similarity_arrays: Dict[int, Dict[int, SimilarityArrays]] = defaultdict(dict)\n    expectation = np.zeros([n_class, n_class])  # S-matrix\n\n    # For each class, we compute the expectation from the samples.\n    # Create a matrix of similarity [n_class, M, n_samples]\n    # https://scikit-learn.org/stable/modules/metrics.html#metrics\n    similarities = lambda k: np.array(pairwise_distances(class_samples[k], data, metric=distance))\n\n    for class_ix in class_samples:\n        # Compute E_{p(x\\mid C_i)} [p(x\\mid C_j)] using a Parzen-Window\n        all_similarities = similarities(class_ix)  # Distance arrays for all class samples\n        all_indices = class_indices[class_ix]  # Indices for all class samples\n        for m, sample_ix in enumerate(all_indices):\n            indices_k = all_similarities[m].argsort()[: k_nearest + 1]  # kNN indices (incl self)\n            target_k = np.array(target)[indices_k[1:]]  # kNN class labels (self is dropped)\n            # Get the Parzen-Window probability\n            probability = np.array(\n                # kNN class proportions\n                [(target_k == nghbr_class).sum() / k_nearest for nghbr_class in range(n_class)]\n            )\n            probability_norm = probability / get_volume(data[indices_k])  # Parzen-window normalized\n            similarity_arrays[class_ix][sample_ix] = SimilarityArrays(\n                sample_probability=probability, sample_probability_norm=probability_norm\n            )\n            expectation[class_ix] += probability_norm\n\n        # Normalize the proportions to facilitate row comparison\n        expectation[class_ix] /= expectation[class_ix].sum()\n\n    # Make everything nice by replacing INF with zero values.\n    expectation[np.logical_not(np.isfinite(expectation))] = 0\n\n    # Logging\n    log.info(\"----------------Diagonal--------------------\")\n    log.info(np.round(np.diagonal(expectation), 4))\n    return expectation, similarity_arrays\n</code></pre> <p>Reference for <code>find_samples</code></p> <p>Find M samples per class Args:     data: [num_samples, n_features], the inputs     target: [num_samples], the classes     n_class: The number of classes     M: (int, float), Number or proportion of sample per class     seed: seeding for sampling.</p> <p>Returns: Selected items per class and their indices.</p> Source code in <code>spectral_metric/lib.py</code> <pre><code>def find_samples(\n    data: np.ndarray, target: np.ndarray, n_class: int, M=100, seed=None\n) -&gt; Tuple[Dict[int, Array], Dict[int, Array]]:\n    \"\"\"\n    Find M samples per class\n    Args:\n        data: [num_samples, n_features], the inputs\n        target: [num_samples], the classes\n        n_class: The number of classes\n        M: (int, float), Number or proportion of sample per class\n        seed: seeding for sampling.\n\n\n    Returns: Selected items per class and their indices.\n\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    class_samples = {}\n    class_indices = {}\n    indices = np.arange(len(data))\n    for k in np.unique(target):\n        indices_in_cls = rng.permutation(indices[target == k])\n        to_take = min(M if M &gt; 1 else int(M * len(indices_in_cls)), len(indices_in_cls))\n        class_samples[k] = data[indices_in_cls[:to_take]]\n        class_indices[k] = indices_in_cls[:to_take]\n    return class_samples, class_indices\n</code></pre>"}]}